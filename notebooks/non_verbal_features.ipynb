{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f99b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
    "from pydantic import BaseModel\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0daba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeakerFeatures(BaseModel):\n",
    "    total_speaking_duration: float\n",
    "    total_turns: int\n",
    "    speech_ratio: float  # total_speaking_duration / conv_length\n",
    "    \n",
    "    # A bunch of statistics on the turn lengths distribution\n",
    "    # --------------------------\n",
    "    mean_turn_duration: float\n",
    "    median_turn_duration: float\n",
    "    std_turn_duration: float\n",
    "    min_turn_duration: float\n",
    "    max_turn_duration: float\n",
    "    percentiles: Dict[str, float]\n",
    "    # --------------------------\n",
    "    \n",
    "    interruptions_made: int\n",
    "    interruptions_received: int\n",
    "    interrupted_by: Dict[str, int]\n",
    "\n",
    "class ConversationMetrics(BaseModel):\n",
    "    num_speakers: int\n",
    "    total_speaking_time: float\n",
    "    overlap_duration: float\n",
    "    silence_duration: float\n",
    "    overlap_ratio: float\n",
    "    silence_ratio: float\n",
    "    total_interruptions: int\n",
    "    interruption_rate: float  # interruptions per minute\n",
    "    \n",
    "class BasicMetricsResponse(BaseModel):\n",
    "    speakers: Dict[str, SpeakerFeatures]\n",
    "    conversation: ConversationMetrics\n",
    "    \n",
    "class SpeakerSegment(BaseModel):\n",
    "    start: float\n",
    "    end: float\n",
    "    speaker: str\n",
    "\n",
    "class DiarizationResponse(BaseModel):\n",
    "    segments: list[SpeakerSegment]\n",
    "    num_speakers: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afab59d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3582d070ad44f848e022bc9d25e27bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Community-1 open-source speaker diarization pipeline\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-community-1\")\n",
    "\n",
    "mps = torch.device(\"mps\")\n",
    "pipeline.to(mps)\n",
    "\n",
    "# apply pretrained pipeline (with optional progress hook)\n",
    "with ProgressHook() as hook:\n",
    "    output = pipeline(\"../data/processed/audios/test_audio.wav\", hook=hook)  # runs locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db0bdb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_diarization_response(output) -> DiarizationResponse:\n",
    "    segments = []\n",
    "    speakers = set()\n",
    "    \n",
    "    # Extract speaker segments from the diarization output\n",
    "    for turn, speaker in output.speaker_diarization:\n",
    "        segment = SpeakerSegment(\n",
    "            start=turn.start,\n",
    "            end=turn.end,\n",
    "            speaker=f\"speaker_{speaker}\"\n",
    "        )\n",
    "        segments.append(segment)\n",
    "        speakers.add(speaker)\n",
    "    \n",
    "    # Create the response object\n",
    "    response = DiarizationResponse(\n",
    "        segments=segments,\n",
    "        num_speakers=len(speakers)\n",
    "    )\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15c245fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DiarizationResponse from pipeline output\n",
    "diarization_result = create_diarization_response(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cbb16be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of speakers: 5\n",
      "Total segments: 279\n",
      "\n",
      "Sample segments:\n",
      "From 10.9s to 13.5s: speaker_SPEAKER_04\n",
      "From 28.0s to 28.7s: speaker_SPEAKER_04\n",
      "From 60.5s to 65.0s: speaker_SPEAKER_04\n",
      "From 70.3s to 92.1s: speaker_SPEAKER_04\n",
      "From 93.7s to 94.2s: speaker_SPEAKER_04\n"
     ]
    }
   ],
   "source": [
    "# Display the result\n",
    "print(f\"Number of speakers: {diarization_result.num_speakers}\")\n",
    "print(f\"Total segments: {len(diarization_result.segments)}\")\n",
    "print(\"\\nSample segments:\")\n",
    "for segment in diarization_result.segments[:5]:  # Show first 5 segments\n",
    "    print(f\"From {segment.start:.1f}s to {segment.end:.1f}s: {segment.speaker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2631348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_metrics(diarization_result: DiarizationResponse,\n",
    "                  audio_length: float,\n",
    "                  percentiles: List[int] = [10, 25, 75, 90]) -> BasicMetricsResponse:\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of conversation dynamics from diarization results.\n",
    "\n",
    "    Returns:\n",
    "        BasicMetricsResponse (pydantic) with 'speakers' and 'conversation' populated.\n",
    "    \"\"\"\n",
    "    # Group segments by speaker and sort all segments chronologically\n",
    "    speaker_segments: Dict[str, List[Tuple[float, float]]] = {}\n",
    "    all_segments: List[Tuple[float, float, str]] = []\n",
    "\n",
    "    for segment in diarization_result.segments:\n",
    "        speaker_segments.setdefault(segment.speaker, []).append((segment.start, segment.end))\n",
    "        all_segments.append((segment.start, segment.end, segment.speaker))\n",
    "\n",
    "    # Sort segments chronologically by start time\n",
    "    all_segments.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Initialize interruption tracking\n",
    "    interruptions = {speaker: 0 for speaker in speaker_segments.keys()}\n",
    "    interrupted_by = {\n",
    "        speaker: {other: 0 for other in speaker_segments.keys() if other != speaker}\n",
    "        for speaker in speaker_segments.keys()\n",
    "    }\n",
    "\n",
    "    # Detect interruptions\n",
    "    active_speakers: Dict[str, float] = {}  # speaker -> end_time\n",
    "\n",
    "    for start, end, speaker in all_segments:\n",
    "        # Check if this speaker is interrupting anyone\n",
    "        for active_speaker, active_end in list(active_speakers.items()):\n",
    "            if active_speaker != speaker and start < active_end:\n",
    "                interruptions[speaker] += 1\n",
    "                interrupted_by[active_speaker][speaker] += 1\n",
    "\n",
    "        # Update active speakers\n",
    "        active_speakers[speaker] = end\n",
    "\n",
    "        # Remove speakers who have finished before current start\n",
    "        active_speakers = {s: e for s, e in active_speakers.items() if e > start}\n",
    "\n",
    "    # Calculate features for each speaker\n",
    "    speaker_features: Dict[str, SpeakerFeatures] = {}\n",
    "    total_speaking_time = 0.0\n",
    "\n",
    "    for speaker, segments in speaker_segments.items():\n",
    "        turn_durations = [end - start for start, end in segments]\n",
    "        total_speaking_duration = sum(turn_durations)\n",
    "        total_speaking_time += total_speaking_duration\n",
    "        total_turns = len(turn_durations)\n",
    "        speech_ratio = total_speaking_duration / audio_length if audio_length > 0 else 0.0\n",
    "\n",
    "        arr = np.array(turn_durations, dtype=float)\n",
    "        mean_turn_duration = float(np.mean(arr)) if arr.size else 0.0\n",
    "        median_turn_duration = float(np.median(arr)) if arr.size else 0.0\n",
    "        std_turn_duration = float(np.std(arr)) if arr.size else 0.0\n",
    "        min_turn_duration = float(np.min(arr)) if arr.size else 0.0\n",
    "        max_turn_duration = float(np.max(arr)) if arr.size else 0.0\n",
    "\n",
    "        percentile_values = np.percentile(arr, percentiles).tolist() if arr.size else [0.0] * len(percentiles)\n",
    "        percentile_dict = {f\"percentile_{p}\": float(v) for p, v in zip(percentiles, percentile_values)}\n",
    "\n",
    "        speaker_features[speaker] = SpeakerFeatures(\n",
    "            total_speaking_duration=float(total_speaking_duration),\n",
    "            total_turns=int(total_turns),\n",
    "            speech_ratio=float(speech_ratio),\n",
    "            mean_turn_duration=mean_turn_duration,\n",
    "            median_turn_duration=median_turn_duration,\n",
    "            std_turn_duration=std_turn_duration,\n",
    "            min_turn_duration=min_turn_duration,\n",
    "            max_turn_duration=max_turn_duration,\n",
    "            interruptions_made=int(interruptions[speaker]),\n",
    "            interruptions_received=int(sum(interrupted_by[speaker].values())),\n",
    "            interrupted_by={k: int(v) for k, v in interrupted_by[speaker].items()},\n",
    "            percentiles=percentile_dict\n",
    "        )\n",
    "\n",
    "    # Conversation-level metrics\n",
    "    # Build timeline and compute coverage\n",
    "    timeline = [(seg.start, seg.end, seg.speaker) for seg in diarization_result.segments]\n",
    "    timeline.sort(key=lambda x: x[0])\n",
    "\n",
    "    total_coverage = 0.0\n",
    "    last_end = 0.0\n",
    "    for start, end, _ in timeline:\n",
    "        if start > last_end:\n",
    "            # Disjoint segment\n",
    "            total_coverage += (end - start)\n",
    "            last_end = end\n",
    "        elif end > last_end:\n",
    "            # Partial overlap\n",
    "            total_coverage += (end - last_end)\n",
    "            last_end = end\n",
    "\n",
    "    overlap_duration = total_speaking_time - total_coverage\n",
    "    silence_duration = max(0.0, audio_length - total_coverage)\n",
    "    total_interruptions = sum(interruptions.values())\n",
    "    interruption_rate = total_interruptions / (audio_length / 60) if audio_length > 0 else 0.0\n",
    "\n",
    "    conversation_metrics = ConversationMetrics(\n",
    "        num_speakers=int(diarization_result.num_speakers),\n",
    "        total_speaking_time=float(total_speaking_time),\n",
    "        overlap_duration=float(overlap_duration),\n",
    "        silence_duration=float(silence_duration),\n",
    "        overlap_ratio=float(overlap_duration / audio_length) if audio_length > 0 else 0.0,\n",
    "        silence_ratio=float(silence_duration / audio_length) if audio_length > 0 else 0.0,\n",
    "        total_interruptions=int(total_interruptions),\n",
    "        interruption_rate=float(interruption_rate)\n",
    "    )\n",
    "\n",
    "    return BasicMetricsResponse(speakers=speaker_features, conversation=conversation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb63499d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "speaker_SPEAKER_04 statistics:\n",
      "  Speaking time: 453.70s (32.8% of conversation)\n",
      "  Number of turns: 119\n",
      "  Average turn duration: 3.81s (median: 1.69s)\n",
      "  Interruptions made: 30\n",
      "  Interruptions received: 33\n",
      "\n",
      "speaker_SPEAKER_01 statistics:\n",
      "  Speaking time: 102.85s (7.4% of conversation)\n",
      "  Number of turns: 31\n",
      "  Average turn duration: 3.32s (median: 0.47s)\n",
      "  Interruptions made: 9\n",
      "  Interruptions received: 14\n",
      "\n",
      "speaker_SPEAKER_03 statistics:\n",
      "  Speaking time: 229.92s (16.6% of conversation)\n",
      "  Number of turns: 83\n",
      "  Average turn duration: 2.77s (median: 0.62s)\n",
      "  Interruptions made: 35\n",
      "  Interruptions received: 15\n",
      "\n",
      "speaker_SPEAKER_00 statistics:\n",
      "  Speaking time: 243.52s (17.6% of conversation)\n",
      "  Number of turns: 41\n",
      "  Average turn duration: 5.94s (median: 1.62s)\n",
      "  Interruptions made: 8\n",
      "  Interruptions received: 23\n",
      "\n",
      "speaker_SPEAKER_02 statistics:\n",
      "  Speaking time: 1.97s (0.1% of conversation)\n",
      "  Number of turns: 5\n",
      "  Average turn duration: 0.39s (median: 0.17s)\n",
      "  Interruptions made: 3\n",
      "  Interruptions received: 0\n",
      "\n",
      "Conversation metrics:\n",
      "  Total speaking time: 1031.97s\n",
      "  Silence: 408.95s (29.5%)\n",
      "  Overlap: 56.92s (4.1%)\n",
      "  Total interruptions: 85 (3.68/minute)\n"
     ]
    }
   ],
   "source": [
    "audio_length = 1384\n",
    "\n",
    "# Get comprehensive analysis results\n",
    "analysis = basic_metrics(diarization_result, audio_length)\n",
    "\n",
    "# Print speaker-specific metrics\n",
    "for speaker, features in analysis.speakers.items():\n",
    "    print(f\"\\n{speaker} statistics:\")\n",
    "    print(f\"  Speaking time: {features.total_speaking_duration:.2f}s ({features.speech_ratio*100:.1f}% of conversation)\")\n",
    "    print(f\"  Number of turns: {features.total_turns}\")\n",
    "    print(f\"  Average turn duration: {features.mean_turn_duration:.2f}s (median: {features.median_turn_duration:.2f}s)\")\n",
    "    print(f\"  Interruptions made: {features.interruptions_made}\")\n",
    "    print(f\"  Interruptions received: {features.interruptions_received}\")\n",
    "\n",
    "# Print conversation-level metrics\n",
    "conv = analysis.conversation\n",
    "print(\"\\nConversation metrics:\")\n",
    "print(f\"  Total speaking time: {conv.total_speaking_time:.2f}s\")\n",
    "print(f\"  Silence: {conv.silence_duration:.2f}s ({conv.silence_ratio*100:.1f}%)\")\n",
    "print(f\"  Overlap: {conv.overlap_duration:.2f}s ({conv.overlap_ratio*100:.1f}%)\")\n",
    "print(f\"  Total interruptions: {conv.total_interruptions} ({conv.interruption_rate:.2f}/minute)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
